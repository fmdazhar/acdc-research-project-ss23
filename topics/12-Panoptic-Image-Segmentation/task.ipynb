{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Task* - Panoptic Image Segmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Develop a methodology to perform panoptic image segmentation and evaluate its performance on semantic segmentation, instance segmentation, and panoptic segmentation tasks.\n",
    "\n",
    "- [Background and Motivation](#background-and-motivation)\n",
    "- [Task](#task)\n",
    "- [Required Tools and Data](#required-tools-and-data)\n",
    "- [Hints](#Hints)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Motivation\n",
    "\n",
    "*Semantic segmentation* in images is a popular visual perception task for automated driving applications. By assigning a semantic class label to each pixel in an image, semantic segmentation can provide a semantic scene understanding of the current vehicle environment. While it is especially helpful for detecting non-object regions like the drivable road space, it cannot differentiate between multiple instances of the same class, e.g., multiple vehicles. The detection and segmentation of object instances is instead known as *instance segmentation*, where the algorithm assigns a unique label to each pixel belonging to a unique object instance. Going one step further, *panoptic segmentation* is the combination of *semantic segmentation* and *instance segmentation*: it assigns both a semantic label to each pixel as well as an instance label to pixels belonging to object instances. This provides a more complete and informative representation of the visual scene than either semantic or instance segmentation alone. By providing a unified representation of the visual scene, panoptic segmentation can help improve the performance of downstream tasks such as object tracking.\n",
    "\n",
    "![](./assets/semantic-instance-panoptic-segmentation.png)\n",
    "[*Image Source*](https://www.mdpi.com/2079-9292/11/12/1884#)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "The task is to develop a methodology to perform panoptic image segmentation and evaluate its performance on semantic segmentation, instance segmentation, and panoptic segmentation tasks.\n",
    "\n",
    "### Subtasks\n",
    "\n",
    "> ***Note:*** *The subtasks listed below do not have to be followed strictly. They serve the purpose of guiding you along your own research for this topic.*\n",
    "\n",
    "1. Search for publicly available datasets for panoptic image segmentation related to driving.\n",
    "2. Research existing panoptic image segmentation methodologies.\n",
    "3. Implement a TensorFlow (or PyTorch) data pipeline for loading and pre-processing a chosen panoptic image segmentation dataset.\n",
    "4. Implement a TensorFlow (or PyTorch) model for panoptic image segmentation.\n",
    "5. Train a model for panoptic image segmentation on the chosen dataset.\n",
    "6. Iterate on the training with different augmentation, generalization techniques, and/or other hyperparameters in order to optimize generalization capabilities of the trained model.\n",
    "7. Qualitatively and quantitatively evaluate the trained model on the training dataset's test data.\n",
    "   - semantic segmentation performance\n",
    "   - instance segmentation performance\n",
    "   - panoptic segmentation performance\n",
    "8. Qualitatively and quantitatively evaluate the trained model on ika's validation dataset (quantitative evaluation only on semantic image segmentation task).\n",
    "9.  Document your research, developed approach, and evaluations in a Jupyter notebook report. Explain and reproduce individual parts of your implemented functions with exemplary data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Tools and Data\n",
    "\n",
    "### Tools\n",
    "\n",
    "- TensorFlow/PyTorch\n",
    "- *(optional)* Image Segmentation Training Pipeline & Model *(see [ACDC Exercise: Semantic Image Segmentation](https://github.com/ika-rwth-aachen/acdc-notebooks/blob/main/section_2_sensor_data_processing/1_semantic_image_segmentation.ipynb))*\n",
    "\n",
    "### Data\n",
    "\n",
    "- [ika's validation dataset](data/ika-dataset/)\n",
    "- *(to be found)* publicly available datasets for panoptic image segmentation related to driving"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "### Relevant ACDC Sections\n",
    "\n",
    "- **Sensor Data Processing Algorithms**\n",
    "  - Image Segmentation\n",
    "\n",
    "### TFDS Datasets\n",
    "\n",
    "[TensorFlow Datasets](https://www.tensorflow.org/datasets) is a collection of datasets ready-to-use with TensorFlow. It may already contain datasets interesting for panoptic image segmentation and thus save you from worrying about parsing data files from disk. All TFDS datasets are exposed as [`tf.data.Dataets`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), ready to be passed to [`model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit).\n",
    "\n",
    "The example below shows how easy it is to load a TFDS dataset, in this case the famous [MNIST dataset](https://www.tensorflow.org/datasets/catalog/mnist). Note that this dataset is not related to semantic image segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.10.0 in /opt/conda/lib/python3.9/site-packages (2.10.0)\n",
      "Requirement already satisfied: tensorflow-datasets==4.4.0 in /opt/conda/lib/python3.9/site-packages (4.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (3.7.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (21.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (4.2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (0.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (2.10.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (1.6.3)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (1.22.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (22.9.24)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (2.10.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (1.1.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (0.27.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (1.49.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (14.0.6)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (1.14.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (2.0.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (3.19.6)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (2.10.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0) (1.16.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets==4.4.0) (0.3.5.1)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets==4.4.0) (1.10.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets==4.4.0) (4.62.1)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets==4.4.0) (0.18.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets==4.4.0) (2.26.0)\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets==4.4.0) (2.3)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets==4.4.0) (21.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow==2.10.0) (0.37.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets==4.4.0) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets==4.4.0) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets==4.4.0) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets==4.4.0) (2021.10.8)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.12.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.8.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (4.6.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.5.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->tensorflow==2.10.0) (2.4.7)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-metadata->tensorflow-datasets==4.4.0) (1.56.4)\n"
     ]
    }
   ],
   "source": [
    "# install required Python packages via pip\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install \\\n",
    "    tensorflow==2.10.0 \\\n",
    "    tensorflow-datasets==4.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 12:01:34.618958: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-13 12:01:34.751233: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-13 12:01:35.240028: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/targets/x86_64-linux/lib:/usr/local//usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-13 12:01:35.240078: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/targets/x86_64-linux/lib:/usr/local//usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-13 12:01:35.240084: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-13 12:01:36.129263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 12:01:36.133901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 12:01:36.134605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 12:01:36.135589: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-13 12:01:36.137462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 12:01:36.138164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 12:01:36.138840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 12:01:36.519141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 12:01:36.519838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 12:01:36.520485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 12:01:36.521115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21603 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:21:00.0, compute capability: 8.6\n",
      "2023-01-13 12:01:36.796654: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TFDS MNIST dataset contains three different splits of data:\n",
      "{'test': <PrefetchDataset element_spec={'image': TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}>, 'train': <PrefetchDataset element_spec={'image': TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}>}\n",
      "The dataset has two features, 'image' and 'label'. Here is one sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_b0d21\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b0d21_level0_col0\" class=\"col_heading level0 col0\" >image</th>\n",
       "      <th id=\"T_b0d21_level0_col1\" class=\"col_heading level0 col1\" >label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b0d21_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b0d21_row0_col0\" class=\"data row0 col0\" >[[[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 84]\n",
       "  [254]\n",
       "  [101]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [174]\n",
       "  [253]\n",
       "  [119]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 31]\n",
       "  [247]\n",
       "  [202]\n",
       "  [ 29]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  1]\n",
       "  [  1]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [141]\n",
       "  [253]\n",
       "  [168]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 66]\n",
       "  [208]\n",
       "  [ 56]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [186]\n",
       "  [253]\n",
       "  [120]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 57]\n",
       "  [253]\n",
       "  [119]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 28]\n",
       "  [249]\n",
       "  [240]\n",
       "  [ 25]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 34]\n",
       "  [253]\n",
       "  [119]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [109]\n",
       "  [254]\n",
       "  [197]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 53]\n",
       "  [253]\n",
       "  [119]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [135]\n",
       "  [254]\n",
       "  [133]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [133]\n",
       "  [254]\n",
       "  [119]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 27]\n",
       "  [240]\n",
       "  [255]\n",
       "  [ 35]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  7]\n",
       "  [235]\n",
       "  [253]\n",
       "  [208]\n",
       "  [151]\n",
       "  [169]\n",
       "  [215]\n",
       "  [253]\n",
       "  [206]\n",
       "  [  2]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 97]\n",
       "  [253]\n",
       "  [253]\n",
       "  [253]\n",
       "  [254]\n",
       "  [253]\n",
       "  [253]\n",
       "  [253]\n",
       "  [ 86]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [150]\n",
       "  [244]\n",
       "  [145]\n",
       "  [119]\n",
       "  [101]\n",
       "  [ 82]\n",
       "  [253]\n",
       "  [253]\n",
       "  [ 14]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 84]\n",
       "  [254]\n",
       "  [172]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [174]\n",
       "  [253]\n",
       "  [119]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [237]\n",
       "  [252]\n",
       "  [ 56]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [ 50]\n",
       "  [241]\n",
       "  [182]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [187]\n",
       "  [254]\n",
       "  [249]\n",
       "  [105]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [186]\n",
       "  [253]\n",
       "  [206]\n",
       "  [ 21]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [227]\n",
       "  [242]\n",
       "  [ 32]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [236]\n",
       "  [219]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]\n",
       "\n",
       " [[  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]\n",
       "  [  0]]]</td>\n",
       "      <td id=\"T_b0d21_row0_col1\" class=\"data row0 col1\" >4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "                                               image  label\n",
       "0  [[[0], [0], [0], [0], [0], [0], [0], [0], [0],...      4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load MNIST\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "ds = tfds.load(\"mnist\")\n",
    "print(\"The TFDS MNIST dataset contains three different splits of data:\")\n",
    "print(ds)\n",
    "print(\"The dataset has two features, 'image' and 'label'. Here is one sample:\")\n",
    "tfds.as_dataframe(ds[\"train\"].take(1))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f35c820e40155ce3a3d6b6baab4aa8cb626eff9596fe63e71a966e5e0dc1513e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
